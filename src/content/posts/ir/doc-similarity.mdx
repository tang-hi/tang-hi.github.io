---
title: Back to Basic - 文档打分及检索优化
pubDate: 2025-01-13
categories: 
  - Full Text Search
  - tf-idf
  - bm25
  - 文本相关性
description: '当用户在搜索框中输入Query后，我们应该如何基于用户的Query来对文档进行排序，
又该如何快速的返回用户所需的结果？ 这篇文章会解释相关的基础知识。'
---

<link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
    integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ"
    crossorigin="anonymous"
/>

当用户进行检索时，他往往希望搜索引擎可以迅速返回他最需要的文档信息。在这个使用场景中，我们需要解决两个问题。
1. 在确定Query的情况下，如何定量的给文档进行打分？
2. 如何在上亿乃至上百亿的文档中快速的找到和用户Query最相关的文档？

下面我们会从这两个问题出发，展示基于文本检索的搜索引擎是如何处理这两个问题(本文只做基础知识的介绍)。

## 从头开始发明TF-IDF和BM25
大多数人看到TF-IDF以及BM25的公式后，就会选择放弃理解公式的含义，而只是将其作为一个给定输入就会产生输出的黑盒来使用(尤其是BM25)。
现在，我们通过再次"发明"的方式来理解这两个公式。

### TF-IDF
在给定Query的情况下，我们如何衡量一篇文档比另一篇文档更加相关呢？一个最显而易见的做法就是，对于一个给定的term，我们可以统计文档中这个term出现的次数。
term出现的次数越多，那这篇文档的相关性越高。

$$
\text{score}(t,D) = \text{occurance}(t,D)
$$

那对于一个Query，我们如何衡量一个文档的相关性呢？我们可以将Query中的每个term的相关性加起来，即
$$
\text{score}(Q,D) = \sum_{t \in Q} \text{score}(t,D)
$$

对于仅仅使用term出现次数来衡量文档的相关性，有两个很明显的问题
1. 我们对于Query中的每个term都是平等对待的，没有考虑哪些term更加重要。
例如你的Query是"猫和老鼠"，检索出来的文档中很有可能是“和”这个词出现的次数最多的文档，这显然不是我们想要的。
2. 文档所包含的term越多，它的相关性就越高，这显然是不合理的。
例如你的Query是"猫和老鼠"，你可能搜出来一个完全无关的文档，仅仅因为这个文档很长，而它恰巧包含了"猫"和"老鼠"这两个词。


为了解决第一个问题，我们需要给予Query中的term一个权重，而一个term的权重应该与他在整个文档集合中的稀缺性成正比。 
如果一个term在很多文档中都出现，说明它是一个常用词，比如"这", "那"。 反之，如果一个term很少在文档中出现，那么它在整个Query中的权重就应该更高。

那么我们如何衡量一个term在整个文档集合中的稀缺性呢？我们可以用包含这个term的文档**倒数**来衡量，即
$$
\text{importance}(t) = \frac{1}{\text{doc\_count}(t)}
$$

为了便于后续处理，我们可以将$$\text{importance}(t)$$ 乘以文档集合的总数进行归一化，即
$$
\text{importance}(t) = \text{(number of documents)} \times \frac{1}{\text{doc\_count}(t)}
$$

至此，我们就从头发明了IDF(Inverse Document Frequency)的概念。

那么我们如何将IDF和TF结合起来呢？我们可以将TF乘以IDF，即

$$
\text{score}(t,D) = \text{occurance}(t,D) \times \text{importance}(t)
                  = \text{tf}(t,D) \cdot \text{idf}(t,D)
$$

至此，TF-IDF的基本概念基本就讲完了。但是我们还需要对上述公式进行一些调整，以便更好的适应实际场景。
我们需要对IDF取log，平滑化处理。具体的公式如下

$$
\text{idf}(t,D) = \log{\frac{N}{|{d \in D : t \in d}|}}
$$

为什么要取`log`呢？ 我们可以从下图中看到，当DF较小时，DF轻微的变化会导致IDF的剧烈变化，这显然是不合理的。
例如，当DF从1变到2时，IDF的数值减少了一半，这显然是不符合常识的，
在一个较大的文档集合中，一个term在两个文档中出现和在一个文档中出现的重要性应该是差不多的。 因此我们需要取`log`使得IDF的变化更加平滑。

![IDF](https://hayesx-1302722143.cos.ap-singapore.myqcloud.com/img/20250113224742.png)


### BM25

在TF-IDF这一节中，我们介绍了TF-IDF的缺点，即会偏向于长文档，以及对于Query中的term没有进行权重的处理。
我们已经解决了Query中的term的权重问题，那么如何解决文档长度的问题呢？

#### 文档长度的问题
在实际的检索过程中，当两篇文档的term出现的次数相同时，我们应该更加青睐于较短的文档。 因此我们需要一个公式来对文档的长度进行惩罚。 
在想出这个公式之前，我们先量化一下文档的长度。我们可以用文档中term的个数来量化文档的长度，即

$$
\text{doc\_length}(D) = \sum_{t \in D} \text{occurance}(t,D)
$$

但是这个仅仅只是文档的长度。我们需要把文档的长度与整个文档集合的平均长度进行比较，从而可以评估在这个文档集合中，这个文档的长度是长还是短。即

$$
\text{doc\_length\_ratio}(D) = \frac{\text{doc\_length}(D)}{\text{avg\_doc\_length}}
$$
在得到能够量化文档长度的公式之后，我们先暂缓一下，思考一下TF-IDF的其他问题。

#### Refine TF
在TF-IDF中，我们假设TF和相关性是成正比的，即TF越大，相关性越高。但是这显然是不合理的。
例如，猫在文档中出现了1000次，那么这篇文档的相关性并不意味比猫在文档中出现了100次的文档高10倍。
在实际搜索中，我们会倾向于认为，当一个term在文档中出现的次数较多时，它相关性的边际效益会递减。

同时如果仅使用TF来衡量文档的相关性，那么对于Query: "猫和老鼠"，出现了"猫"和"老鼠"各一次的文档与出现了"猫"2次的文档的相关性是一样的，
这显然也是不合理的。

因此，我们需要对TF进行调整，使得TF较小时，相关性的增长较快，而TF较大时，相关性的增长较慢。并且对于Query中的term，命中的term越多，相关性越高。
新的TF公式如下
$$
\text{new\_tf}(t,D) = \frac{\text{occurance}(t,D) }{\text{occurance}(t,D) + K}
$$

从图中我们可以看到，当我们加上K之后，TF的增长速度会随着TF的增加而减缓。
![TF](https://hayesx-1302722143.cos.ap-singapore.myqcloud.com/img/20250113232106.png)

同时如果Query为"猫和老鼠"，那么出现了"猫"和"老鼠"各一次的文档, 它的相关性为 $$\frac{1}{2} + \frac{1}{2} = 1$$ (假设K=1)，
而出现了"猫"2次的文档，它的相关性为 $$\frac{2}{3} < 1$$，

到了这里还记得，我们上一节暂缓的文档长度问题吗？我们可以将文档长度的问题与TF的问题结合起来，即将我们在上一节得到的量化文档长度的公式与新的TF公式结合起来，即

$$
\text{new\_tf}(t,D) = \frac{\text{occurance}(t,D) }{\text{occurance}(t,D) + K \cdot \frac{\text{doc\_length}(D)}{\text{avg\_doc\_length}} }
$$

为什么这个公式是合理的呢？如果文档的长度等于平均长度，那么这个公式就等价于我们上一节得到的新的TF公式。
而当文档的长度远大于平均长度时，这个公式会增大K值，从而减小相关性，即对文档的长度进行了惩罚。而当文档的长度远小于平均长度时，这个公式会减小K值，从而增大相关性，即对文档的长度进行了奖励。

#### 自定义文档长度的惩罚
在实际检索的场景中，我们并不总是希望文档的长度越短越好，有时候我们希望文档的长度越长越好。例如在搜索引擎中，我们希望返回的文档尽可能的丰富，这时我们可以自定义一个参数b，来调整文档长度的惩罚。即
$$
(1 - b + b \cdot \frac{\text{doc\_length}(D)}{\text{avg\_doc\_length}}) (0 \leq b \leq 1)
$$

当`b=1`时，公式如下所示，与之前的公式等价
$$
(\frac{\text{doc\_length}(D)}{\text{avg\_doc\_length}})
$$

当`b=0`时，文档的长度不再对相关性产生影响。
$$
(1 - 0 + 0 \cdot \frac{\text{doc\_length}(D)}{\text{avg\_doc\_length}}) = 1
$$

因此用户可以通过调整B的值来调整文档长度对相关性的影响。至此把我们的公式整合一下，即
$$
\text{score}(t,D) = \frac{tf(t,D)}{tf(t,D) + k_1 \cdot (1 - b + b \cdot \frac{\text{doc\_length}(D)}{\text{avg\_doc\_length}})} \cdot \text{idf}(t,D)
$$

#### Refine IDF
BM25中定义的IDF与我们之前定义的IDF有所不同，具体的公式如下
$$
\text{idf}(t,D) = \log{\frac{N - n(t) + 0.5}{n(t) + 0.5}}
$$

这是数学家出于理论上的考虑，对IDF进行了一些调整，使得IDF的变化更加平滑，同时避免了一些极端情况的发生。但是在实际情况中，Lucene对IDF进行了一些调整，具体的公式如下
$$
\text{idf}(t,D) = \log{\frac{ 1 + (N - DF + 0.5)}{DF + 0.5}}
                \approx \log{1 + \frac{N - DF}{DF}}
                = \log{\frac{N}{DF}}
$$
即我们之前定义的IDF公式。

#### Assemble
将我们之前定义的TF，IDF，文档长度的惩罚，文档长度的奖励整合起来，即
$$
\text{BM25}(t,D) = \frac{tf(t,D)}{tf(t,D) + k_1 \cdot (1 - b + b \cdot \frac{\text{doc\_length}(D)}{\text{avg\_doc\_length}})} \cdot \text{idf}(t,D)
$$
一个简单的BM25的公式，通过巧妙的数学，成为了文档相关性打分的事实标准。即使在深度学习的时代，BM25仍然是搜索引擎中最常用的打分公式之一。


## 快速求解TopK
在实际的搜索引擎中, 我们最常处理的Query就是TopK的问题，即给定一个Query，我们需要返回与Query最相关的K个文档。

那么在几十上百亿的文档中，如何快速的找到与Query最相关的K个文档呢？我们当然可以对所有被召回的文档进行打分，然后排序，取TopK。
但是这样的延迟显然是不可接受的。因此我们需要一些技巧来加速这个过程。
这一章会介绍当前最常用的几种技术(无损检索，即检索的TopK文档是准确的)。

> 我们主要关注OR Query， 即Query中的term之间是或的关系。 AND Query 因为求交的特性，往往计算要求不高

### DAAT(Document At A Time)
DAAT是一种朴素的方法，我们会使用一个堆用来维护当前的TopK文档。
它的特点是，对于每一个文档，我们都会计算它的得分，然后与堆顶的文档进行比较，如果得分更高，那么我们就将堆顶的文档替换为当前文档。
具体步骤如下
1. 对于Query中的每一个term，我们检索出该term对应的倒排列表
2. 我们将这些倒排列表合并，得到一个包含所有文档ID的列表
3. 对于每一个文档ID，我们计算它的得分，然后与堆顶的文档进行比较，如果得分更高，那么我们就将堆顶的文档替换为当前文档。

我们通过如下的伪代码来描述这个过程

```python
def DAAT(query, document_collection):
  """
  Performs Document-At-A-Time (DAAT) retrieval.

  Args:
    query: A list of query terms (strings).
    document_collection: collection of documents to be searched.

  Returns:
    A list of (documentID, score) tuples, ranked in descending order of score.
  """

  inverted_index = document_collection.get_inverted_index()
  postings_lists = [inverted_index[term] for term in query]

  merged_postings = merge_postings_lists(postings_lists)
  for doc_id in merged_postings:
    score = compute_score(query, doc_id, document_collection)
    # Update the heap with the new score.
    if len(heap) < k:
      heapq.heappush(heap, (score, doc_id))
    elif score > heap[0][0]:
      heapq.heappushpop(heap, (score, doc_id))
```

### TAAT(Term At A Time)
TAAT 与 DAAT一样，也是一种朴素的方法，它的特点是，我们并不会像DAAT一样，使用堆来维护TopK文档，我们会遍历每一个倒排链，计算出文档的部分得分(partial score)。
我们在内存中维护每一个文档的部分得分，当我们遍历完所有的倒排链之后，我们再对文档的得分进行排序，取TopK。它的具体步骤如下
1. 对于Query中的每一个term，我们检索出该term对应的倒排列表
2. 对于每一个倒排列表，我们遍历其中的每一个文档，计算它的部分得分
3. 对于每一个文档，我们将它的部分得分加到它的总得分上
4. 对于所有的文档，我们对它们的得分进行排序，取TopK

我们通过如下的伪代码来描述这个过程

```python
def TAAT(query, document_collection):
  """
  Performs Term-At-A-Time (TAAT) retrieval.

  Args:
    query: A list of query terms (strings).
    document_collection: collection of documents to be searched.

  Returns:
    A list of (documentID, score) tuples, ranked in descending order of score.
  """

  inverted_index = document_collection.get_inverted_index()
  postings_lists = [inverted_index[term] for term in query]

  scores = {}
  for postings_list in postings_lists:
    for doc_id in postings_list:
      score = compute_score(query, doc_id, document_collection)
      scores[doc_id] += score

  top_k = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]
  return top_k
```

### Compare DAAT and TAAT
在介绍完最简单的两个朴素方法之后，我们来比较一下这两种方法的优缺点。
在实际检索的场景下，我们往往会使用DAAT，不仅因为它的延迟更低，而且它的内存占用更低。TAAT的优势在于， 他对倒排链的顺序读取，有利于CPU的cache命中以及磁盘的预读取。
但是随着文档集合的增大，TAAT的内存占用会迅速增加，而他顺序读取的优势也会因为更新文档的得分而减弱。
而DAAT的内存占用是固定的，且在大规模文档集合中，它的延迟会比TAAT更低。因此在实际的搜索引擎中，我们往往会使用DAAT。

### WAND(Weak AND)
WAND是一种基于DAAT的优化方法，它的特点是，我们会使用一个阈值来剪枝，当文档的得分低于阈值时，我们就不再计算它的得分。
他的具体步骤如下:

索引构建阶段：
1. 在索引构建阶段，我们会记录每一个term所能得到的最高得分(upper bound)，即
    $$\text{upperbound}(t) = max(\text{score}(t, \forall d \in D))$$

索引检索阶段:
1. 对于Query中的每一个term，我们检索出该term对应的倒排列表,设置阈值为0
2. 我们将倒排链表按照`Doc ID`进行排序
3. 我们按顺序遍历每一个倒排链表，将term对应的upperbound进行累加，直到得分超过当前维护的阈值。
3.1 如果遍历完了所有的倒排链表，得分仍然没有超过阈值，那么检索结束，返回结果
3.2 如果得分超过了阈值，那么我们获取这条倒排链的Doc ID
3.2.1 如果这个Doc ID le 当前记录的Doc ID，那么我们任意选择一个之前的倒排链，调用`next(docid)`方法, jump to 1
3.2.2 检查第一条倒排链的Doc ID是否等于当前记录的Doc ID，如果是，那么我们就找到了一个文档，我们计算它的得分，然后与堆顶的文档进行比较，如果得分更高，那么我们就将堆顶的文档替换为当前文档。
3.2.3 如果第一条倒排链的Doc ID不等于当前记录的Doc ID，那么我们任意选择一个之前的倒排链，调用`next(docid)`方法, jump to 1

我们通过如下的伪代码来描述这个过程

```python
def find_pivot_term(term, threshold, postings_lists):
  """
  Find the pivot term for WAND retrieval.

  Args:
    term: A query term (string).
    threshold: The threshold score.

  Returns:
    A tuple (posting_idx, term) of the pivot term.
  """
  score = 0
  for i, postings_list in enumerate(postings_lists):
    term = postings_list.term
    score += term.upperbound
    if score > threshold:
      return i, term

def WAND(query, document_collection):
  """
  Performs Weak AND (WAND) retrieval.

  Args:
    query: A list of query terms (strings).
    document_collection: collection of documents to be searched.

  Returns:
    A list of (documentID, score) tuples, ranked in descending order of score.
  """

  inverted_index = document_collection.get_inverted_index()
  postings_lists = [inverted_index[term] for term in query]
  top_k = []
  threshold = 0
  current_doc_id = None
  while True:
    sort(postings_lists, key=lambda x: x.current_doc_id())
    pivot_idx, pivot_term = find_pivot_term(postings_lists, threshold)
    pivot_doc_id = postings_lists[pivot_idx].current_doc_id()
    if pivot_term is None:
      break
    if pivot_doc_id <= current_doc_id:
      random_postings_list = random.choice(postings_lists[:pivot_idx])
      random_postings_list.next(current_doc_id + 1)
      continue
    else:
      if postings_list[0].current_doc_id() == pivot_doc_id:
        current_doc_id = pivot_doc_id
        score = compute_score(query, current_doc_id, document_collection)
        if len(top_k) < k:
          heapq.heappush(top_k, (score, current_doc_id))
        elif score > top_k[0][0]:
          heapq.heappushpop(top_k, (score, current_doc_id))
          threshold = top_k[0][0]
      else:
        random_postings_list = random.choice(postings_lists[:pivot_idx])
        random_postings_list.next(pivot_doc_id)
```

下面我们在用一个例子来详细说明WAND的过程
### MaxScore
MaxScore是一种基于DAAT的优化方法，它的特点是，我们会使用一个阈值来剪枝，当文档的得分低于阈值时，我们就不再计算它的得分。